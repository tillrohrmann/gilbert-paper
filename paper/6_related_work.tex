%!TEX root=paper.tex
\section{Related Work}
\label{sec:relatedWork}

{\em SystemML}~\cite{ghoting:2011a,Boehm2014,Boehm2014Hybrid,Schelter2015,Elgohary2016} aims to make machine learning algorithms run on massive datasets without burdening the user with low-level implementation details and tedious hand-tuning. 
Therefore, it provides an R-like declarative high-level language, called Declarative Machine learning Language (\emph{DML}), and compiles and optimizes the resulting programs to distributed dataflow systems. 
The linear algebra operations are translated into a directed acyclic graph of high-level operators. 
This abstract representation allows to apply several optimizations such as algebraic rewrites, choice of internal matrix representation and cost-based selection of the best execution plan. 
Afterwards these plans are translated to low-level operators and executed either in the driver memory or distributedly. 
SystemML is one of the main inspirations for Gilbert. 
While it originally only supported MapReduce as runtime, it has recently also moved to supporting more advanced dataflow systems. 
Gilbert differs from SystemML by its fixpoint operator and by leveraging the general optimizers of the underlying system (e.g., the Flink optimizer for matrix multiplication optimization, see~Section~\ref{sec:LinearAlgebraOperations}). 

{\em Samsara}~\cite{Lyubimov2016,Schelter2016} is a domain-specific language for declarative machine learning in cluster environments. 
Samsara allows its users to specify programs using a set of common matrix abstractions and linear algebraic operations, similar to R or MATLAB.
Samsara automatically compiles, optimizes and executes these programs on distributed dataflow systems. 
With Samsara mathematicians and data scientists can leverage the scalability of distributed dataflow systems via common declarative abstractions, without the need for deep knowledge about the programming and execution models of such systems. 
Samsara is part of the Apache Mahout library~\cite{mahout:2011a} and supports a variety of backends, such as Apache Spark or Apache Flink. 

{\em Ricardo}~\cite{das:2010a} uses existing technologies to implement a scalable system for statistical analysis: it executes the data shipment via Hadoop and runs the actual analytic computation by R. 
It integrates these systems via Jaql~\cite{beyer:2011a}, a declarative high-level language for data-processing on Hadoop. 
Thereby, the user can initiate the distribution, transformation and aggregation of data within Hadoop. 
Furthermore, the system supports to run R code on the worker nodes as data transformations. 
However, in this setting the user still has to explicitly specify the data-flow and the data distribution, which requires a substantial understanding of MapReduce's principles. 
{\em RHIPE}~\cite{guha:s2012a} also tries to extend R to scale to large data sets using Hadoop. 
RHIPE follows an approach called divide and recombine. 
The idea is to split the examined data up into several chunks so that they fit in the memory of a single computer. 
Next a collection of analytic methods is applied to each chunk without communicating with any other computer. 
After all chunks are evaluated, the results will be recombined in an aggregation step to create the overall analytic result. 
However, this strict execution pipeline constrains the analysis process considerably. 
A system which integrates more seamlessly into the R ecosystem is {\em pR} (parallel R)~\cite{samatova:2009a}. 
The goal is to let statisticians compute large-scale data without having to learn a new system. 
pR achieves its goal by providing a set of specialized libraries which offer parallelized versions of different algorithms using MPI.
However, MPI does not integrate well with an evironment where clusters are shared and usually execute several jobs concurrently. 
Furthermore, it lacks important properties such as fault-tolerance, elasticity and reliability. 
Another related system for R is {\em rmpi}~\cite{rmpi}. 
As this is only a wrapper for the respective MPI calls, it also suffers from the aforementioned problems. 
Furthermore, there is the SparkR~\cite{Venkataraman2016} project which aims to integrate Spark's API and SQL processing capabilities into R. 

For the other popular numerical computing environment out there, namely MATLAB, also a set of parallelization tools exists. 
The most popular are the MATLAB Parallel Computing Toolbox~\cite{parallelComputingToolbox} and MATLAB Distributed Computing Server~\cite{distributedComputingServer}, which are developed by MathWorks. 
The former permits to parallelize MATLAB code on a multi-core computer and the latter scales the parallelized code up to large cluster of computing machines. 
In combination, they offer a flexible way to specify parallelism in MATLAB code. 
Besides these tools there are also other projects which try to parallelize MATLAB code. 
The most noteworthy candidates are pMatlab~\cite{bliss:ijhpca2007a} and MatlabMPI~\cite{kepner:jpdc2004a} which shall be named here for the sake of completeness. 
Unfortunately, none of these approaches is known to integrate well with the current JVM-based Hadoop ecosystem, which is becoming the main source of data in production deployments. 
Another promising research direction is the deep embedding of the APIs of dataflow systems in their host language~\cite{Alexandrov2015}, where the potential to extend this embedding and the resulting optimizations to linear algebraic operations is currently explored~\cite{Kunft2016}.