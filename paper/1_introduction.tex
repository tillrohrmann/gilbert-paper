%!TEX root=paper.tex
\section{Introduction}

In order to develop Big Data solutions, one has to solve multifaceted problems.
The necessary technology stack comprises the recording of data, the data cleaning, the meta-data generation, the representation and integration with other data sources, the analysis and modeling of the actual problem as well as the interpretation.
Each task for itself is highly complex and deserves an individual paper.
For us, the data analysis part is the key aspect in solving Big Data problems.
Henceforth, we will concentrate on how to gain valuable insights from a huge data set.

Once big data sets were amassed, people quickly recognized that these sets contain valuable information they only have to harness.
Unfortunately, the search for useful patterns and peculiarities strongly resembled the search for a needle in a haystack.
The quest requires sophisticated tools being able to analyze the vast amounts of data.
Most of these tools are based on statistics to extract interesting features.
It is beneficial to apply these statistical means to the complete data set instead of smaller chunks.
Even if the chunks cover the complete data set, important correlations between data points might get lost in smaller parts, if treated individually.
Moreover, the statistical tools improve their descriptive and predictive results by getting fed more data.
The more data is available, the more likely it is that noise cancels out and that the significant patterns manifest.
However, these benefits come at the price of an increased computing time, which requires fast computer systems to make computations feasible.

The insights gained from collected data already help to govern business decisions, improve life quality or simply create new industries from scratch.
For example, retail stores analyze their sales, customer, pricing and weather data in order to decide which products to offer or when to do a discount sale.
That not only increases the revenue of the shops but also boosts the satisfaction of the customers by getting better offers. 
Police departments try to detect probable crime sites by extracting patterns from previously recorded criminal acts and then reinforce the policemen in that region~\cite{lohr:yt2012a}.
The intelligent deployment of policemen improves security for citizens without having to hire more officers.
Hospitals analyze their patients' records and scientific studies in order to find the cancer treatment best complying with the specifics of the patient.
The individual therapy maximizes the chance of cure~\cite{watson:2013a}.
Another example showing the benefits of data analysis made it even into a Hollywood film.
The film Moneyball is based on the true story of the Oakland Athletics baseball team, whose management built an elite team in an unfavorable financial situation.
They employed sophisticated data analysis methods to spot underestimated players, who they could cheaply recruit.
That marked the start of sports statistics, which are nowadays a common tool for professional teams.
These examples emphasize the importance and utility of information gained with analytic tools from gathered data.
Interestingly, the IDC, an international market research firm, estimates that only $0.5\%$ of the globally collected data is harnessed~\cite{gantz:iaf2012a}.
They further state that about $23\%$ of today's data is worth being analyzed.
Thus, there is still potential left for improvements.

What are the reasons for the huge gap between actual and dormant exploitation?
One of the reasons is that we are lacking the tools to keep up the pace of how fast data is created and collected.
Our analyzing methods do not scale well for the vast data sets.
The more data a system has to process, the longer it will take to finish.
Since the data is growing at an exponential rate, our analytic capacities have to improve at a similar speed to keep computations feasible.

In order to decrease the runtime of our analytic tools there are two adjusting screws.
First of all, there are the algorithms.
By finding an algorithm with a lower runtime complexity than an existing algorithm, e.g., for clustering, we can drastically decrease the required time.
However, it is strongly doubted that it is possible to develop better algorithms for certain problems.
In some cases, it is even proven that there exists a lower bound for any algorithm solving the problem.
Thus, certain problems have an inherent limitation of how fast they can be computed.

The other way to speed up the analytic tools is to make the computer faster by vertical or horizontal scaling.
To scale vertically means that we add more resources to a single computer.
For example, the main memory or the frequency of a computer could be increased, giving them more computational power.
In contrast to that, horizontal scaling means to add more computer nodes to a system.
By having more than one node, the work can be split up and distributed across the nodes.
Since each split is smaller than the original problem, the computer can process it faster.

In recent years, we have seen that clock rates of CPUs stagnated.
Before, there was a simple receipt to increase the computing power of micro-controllers; increase the clock rate, which demands more power, and shrink the channel widths to mitigate for the increased power consumption.
However, the shrinkage induces the problem of leakage, which increases the power demand again.
The consumed power is limited by the amount of energy that can be dissipated and thus there is a technological limit for the increase of clock rate.
When it became clear that the micro-controller would hit this so-called ``power wall'', one duplicated the micro-controller's functionality to support simultaneous execution of multiple applications and to harness the inherent parallelism of programs.

The emerging multi-core and distributed systems pose new challenges for programmers, since now they have to know about locking, deadlocks, race-conditions and inter-process communication in order to make most of the available hardware.
Since they have to be able to reason about interwoven parallel control flows, parallel program development is highly cumbersome and error-prone.
Therefore, new programming models are conceived, a development that relieves the programmer from the tedious low-level tasks related to parallelization such as load-balancing, scheduling of parallel tasks and fault recovery.
The programmer can concentrate on the actual algorithm and the goal he wants to achieve by using these new paradigms.

These are the reasons why Google's MapReduce~\cite{dean:c2008a} framework and its open source re-implementation Hadoop~\cite{hadoop:2008a} became so popular among scientists as well as engineers.
MapReduce is a programming framework for concurrent computations on vast amounts of data running on a large cluster.
Its ingenious idea was to separate the computation into a \emph{map} and a \emph{reduce} phase.
In the map phase, the input data set is split into elements which are all processed independently.
Afterwards, the results of the mapper are grouped together and each produced group is given to a reducer.
The reducer knows about all elements in his group and produces the final result.
The strengths of MapReduce are that it is expressive enough to implement a multitude of different algorithms while facilitating at the same time parallel execution.

But still, MapReduce and other frameworks force the user to express the program in a certain way, which is often not natural or intuitive to a user coming from a different domain.
Especially, in the field of data analytics and machine learning programs are usually expressed in a mathematical form.
Therefore, systems such as MATLAB~\cite{matlab} and R~\cite{r:1993a} are widely used and recognized for their fast prototyping capabilities and their extensive mathematical libraries.
However, these linear algebra systems lack proper support for automatic parallelization on large clusters and are thus restricting the user to a single workstation.
Therefore, the amount of processible data is limited to the size of the main memory, which constitutes a serious drawback for real-world applications.

As a solution we propose Gilbert, a distributed sparse linear algebra environment.
Gilbert allows to write MATLAB-like code and execute it on massively parallel dataflow systems.
Its expressiveness allows to quickly develop scalable algorithms to analyze web-scale data.
We introduce a novel fixed-point operator which replaces loops by recursion and which can be efficienlty parallelized.
Furthermore, we demonstrate that dataflow optimizers can be used to automatically select an optimal matrix multiplication strategy.

The rest of the work is structured as follows.
In \cref{sec:overview} we give an overview of Gilbert.
Then, in \cref{sec:gilbertFeatures}, Gilbert's language and its features are described.
\cref{sec:gilbertRuntime} presents how linear algebra operations are mapped to a dataflow system.
Gilbert's performance and scalability is evaluated in \cref{sec:evaluation}.
Related work is covered in \cref{sec:relatedWork} before the work is concluded in \cref{sec:conclusion}.