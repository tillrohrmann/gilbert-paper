\section{Gilbert Language}
\label{cha:gilbertlanguage}

Prior to the actual start of development of Gilbert, we had to decide which higher-level language Gilbert should support.
Since Gilbert aims to be a sparse linear algebra environment, R and MATLAB are the natural candidates.
In fact, R and MATLAB are quite similar and are both widely used in academics as well as industry to develop mathematical programs.
Due to their expressiveness and their rich library support with myriads of mathematical functions, they are just as well suited for quick prototyping as it is for full-fledged development.
In combination with their ease of use, they quickly became the standard in industry for numerical processing.
Our final choice fell on MATLAB, because of our higher familiarity with this language.
Yet, we believe that there are no fundamental reasons which would prohibit the support of R.
In fact, the translation from one language to the other should be straight-forward on the compiler level.

Another reason to imitate an existing language and not to devise a new language geared towards parallel execution is the laziness of people.
Since its part of human nature to be sluggish, people are initially unwilling to learn new things or to re-adapt.
Therefore, Gilbert was conceived to require as little re-adaption as possible of users familiar with MATLAB.
We believe that this aspect is a crucial property of Gilbert in order to be successfully adopted by the statistical community.
Furthermore, a neat side effect is that existing MATLAB code can almost seamlessly be ported to Gilbert and thus benefits instantaneously from the computational power of a large shared nothing cluster.
This feature becomes particularly relevant considering the huge existing code base.
Re-coding parts of it would be prohibitive, because of the required labor and provoked costs.

In the following sections, we will describe the key aspects of the Gilbert language and explain the design decisions taken.
Subsequently, we will give a formal specification of the Gilbert language and describe the used parser.

\subsection{Language Features}
\label{sec:languageFeatures}

In order to not be overwhelmed by the complexity of MATLAB but still support a full functional subset of it, Gilbert was restricted to support the following language features and data types.
We decided to keep the feature set as small as possible to concentrate on the parallelization instead of supporting hardly used language features.
The elementary data type of linear algebra and therefore also of MATLAB is a matrix.
Gilbert supports arbitrary $2$-dimensional matrices whose elements can be \emph{double} or \emph{boolean}.
Vectors are not represented by a special type but instead are treated as a matrix.
Additionally, scalar \emph{double} and \emph{boolean} values are supported.
The reason to introduce a \emph{boolean} type is the control flow of loops, being covered in a later paragraph.
Since Gilbert has to interact with data stored on disk, strings are supported.

Gilbert also implements cell array types.
A cell array consists of indexed data containers, called cells.
Each cell can contain an individual data type.
Therefore, cell arrays can be used to pass multiple data items combined as one argument to a function or to obtain multiple items from a function.
The latter aspect is particularly important, because Gilbert only supports functions with a single return value.
A cell array is defined using curly braces and commas to separate individual values.
The cells can be accessed by an index appended in curly braces to the cell array variable.
\Cref{lst:cellArray} shows how to define and access them.
The formal definition of the syntax of cell arrays can be found in \cref{sec:languageGrammar}.
In contrast to MATLAB's cell arrays, though, Gilbert only allows to have $1$-dimensional arrays.
However, this constraint does not impose a serious restriction, since all multi-dimensional arrays can be transformed into $1$-dimensional arrays. 

\begin{listing}[!h]
  \begin{CenteredBox}
    \begin{lstlisting}[language=Matlab,
        commentstyle=\color{black},
        stringstyle=\color{black},
    ]
c = {true, 2*2, 'cell', 'array'};
b = c{1} & false; % = false
d = c{2} ^ 2; % = 16
s = {c{4}, c{3}}; % = {'array', 'cell'} 
    \end{lstlisting}
  \end{CenteredBox}
  \caption{Cell array usage in Gilbert. Definition of a 4 element cell array which is accessed subsequently.}
  \label{lst:cellArray}
\end{listing}

Gilbert supports the basic linear algebra operations defined on matrices and scalars.
They include among others the common operations \code{+}, \code{-}, \code{/} and \code{*}, whereas \code{*} denotes the matrix-matrix multiplication and all other operations are interpreted cellwisely.
The cellwise multiplication is indicated by a point preceding the operator as it is common for cellwise operations in MATLAB.
Gilbert also supports comparisons operators such as \code{>}, \code{>=}, \code{==} and \code{\textasciitilde=}.
These operators become handy for realizing dynamic termination criteria for loops.
The full set of supported mathematical operations can be found in appendix 1.

Besides the basic arithmetic operations, the user can also define named functions and anonymous functions.
This feature not only enables a better structure of the code but also allows to express second-order functions.
The syntax of anonymous functions adheres to the MATLAB syntax.
An anonymous function which calculates the sum of its input squares could be defined as follows: \code{@(x,y) x*x + y*y}.
A formal definition can be found in \cref{sec:languageGrammar}.

An important aspect of MATLAB are loops.
MATLAB permits the user to express \code{for} and \code{while} loops.
However, these loops are quite powerful in the sense that they allow iterations with side effects.
The problem of parallelization of iterations with side effects is that the referenced external state has to be maintained.
This circumstance makes preprocessing and execution unnecessarily complex.
For the sake of simplicity, Gilbert is limited to a different loop mechanism.
Gilbert offers a fixpoint operator \code{fixpoint}, which iteratively applies a given update function $f$ on the previous result of $f$, starting with an initial value $x$ at iteration $0$.
Thus, the $n^{th}$ iteration is equivalent to applying the function $f$ $n$ times to $x$: 
\begin{displaymath}
  n^{th}\text{ iteration}\equiv\underbrace{f(f(\ldots(f(x))\ldots))}_{\text{$n$ times}}
\end{displaymath}

In order to terminate the fixpoint operation, the operator provides two mechanisms.
First of all, the user has to specify a maximum number \code{m} of iterations after which the loop is terminated.
This mechanism is henceforth denoted as the \emph{static termination criterion}.
Such a termination criterion is often not sufficient for machine learning algorithms, because they usually continue their computations until they have reached a certain convergence criterion.
Since the number of iterations necessary to reach this state is not always known a priori, it is called the \emph{dynamic termination criterion}.
Gilbert offers support for the dynamic termination criterion.
The user can provide a convergence function \code{c} to the fixpoint operator.
The convergence function is called with the previous and current fixpoint value and returns a boolean value, indicating whether the termination criterion has been fulfilled or not.
Thus, the fixpoint operator terminates either if convergence was detected or if the maximum number of iterations is exceeded.
Consequently, the fixpoint operator is defined as follows:
\begin{equation}
fixpoint: \underbrace{T}_{\text{\code{x}}} \times \left( \underbrace{T \rightarrow T}_{\text{\code{f}}} \right) \times \underbrace{\mathbb{N}}_{\text{\code{m}}} \times \left(\underbrace{T\times T \rightarrow \mathbb{B}}_{\text{\code{c}}} \right) \rightarrow T
\label{eqn:fixpoint}
\end{equation}
with $T$ being a generic type variable.

In fact, the fixpoint operator replaces iterations by recursions whereas the update function $f$ is pure.
At this point Gilbert breaks with existing MATLAB code.
To make Gilbert a real subset of MATLAB, the fixpoint operator would have to be integrated into MATLAB.
The integration could easily be done by providing an external library with the definition of the fixpoint function.

Even though the iteration operator restricts the set of valid programs in Gilbert, it is still expressive enough to support a wide variety of programs.
Moreover, all MATLAB programs can be transformed so that the for and while loops are replaced by the fixpoint operator.
This replacement is simply achieved by passing all data that is read or written to as parameters to the update function.
The update function returns the same set of variables, just with updated values.
Such an exemplified transformation can be seen in \cref{fig:for2Fixpoint}.
Here we can see that all data that is operated on is passed to the fixpoint operator as a cell array.
Lines $1-2$ of \cref{fig:for2Fixpoint:fixpoint} define the update function \code{f}.
The parameter \code{x} is a cell array whose first entry contains the accumulator \code{A} and second entry is the loop counter \code{i}.
In line $2$ we see the returned cell array value of the anonymous function.
The first entry is the sum of the current accumulator value and the loop counter.
The second entry is the incremented loop counter for the next iteration.
Line $3$ calls the fixpoint operator with the initial cell array value, the update function and the maximum number of iterations.
The final result is retrieved from the final cell array value in line $4$.

\begin{listing}
  \centering
  \begin{sublisting}{.4\linewidth}
    \begin{lstlisting}[language=Matlab,
      commentstyle=\color{black},
      stringstyle=\color{black},
      keywordstyle=\color{black}\bfseries,
    ]
A = 0;
for i = 1:10
  A = A + i;
end
    \end{lstlisting}
    \caption{For loop}
    \label[listing]{fig:for2Fixpoint:for}
  \end{sublisting}
  \begin{sublisting}{.5\linewidth}
      \begin{lstlisting}[language=Matlab]
f = @(x) ...
  {x{1} + x{2}, x{2} + 1};
r = fixpoint({0,1}, f, 10);
A = r{1};
    \end{lstlisting}
    \caption{Fixpoint}
    \label[listing]{fig:for2Fixpoint:fixpoint}
  \end{sublisting}
  \caption{Transformation from Matlab for loop \subref{fig:for2Fixpoint:for} to Gilbert fixpoint \subref{fig:for2Fixpoint:fixpoint} formulation. Essentially, all iteration data is combined and passed as a cell array value to the update function.}
  \label{fig:for2Fixpoint}
\end{listing}

\subsection{Language Grammar}
\label{sec:languageGrammar}

We could not find an official specification of the MATLAB language.
Therefore, we defined our own grammar with the goal to imitate MATLAB as closely as possible.
Even though we extensively tested the conformity of the language, we cannot guarantee that it is completely equivalent.
In the following, we give the Backus-Naur form (BNF) of the Gilbert language.

%\setlength{\grammarindent}{12em} % increase separation between LHS/RHS
\begin{lstlisting}
<program> ::= <stmtOrFuncList>

<stmtOrFuncList> ::= (<stmt> | <funcDef>)*

<stmt> ::= <assignment> | <expression>

<assignment> ::= <lhs> '=' <rhs>

<lhs> ::= <identifier>

<rhs> ::= <expression>

<funcDef> ::= 'function' <funcValues>? <identifier> <funcParams> <funcBody> 'end'

<funcValues> ::= <identifier> '='
\alt '[' <identifier> (',' <identifier>)* ']' '='

<funcParams> ::= '(' ')'
\alt '(' <identifier> (',' <identifier>)* ')'

<funcBody> ::= <stmtOrFuncList>

<expression> ::= <aexp1>

<aexp1> ::= <aexp2> ('||' <aexp2>)*

<aexp2> ::= <aexp3> ('\&\&' <aexp3>)*

<aexp3> ::= <aexp4> ('|' <aexp4>)*

<aexp4> ::= <aexp5> ('\&' <aexp5>)*

<aexp5> ::= <aexp6> (<compOp> <aexp6>)*

<aexp6> ::= <aexp7> (':' <aexp7>)*

<aexp7> ::= <aexp8> (<addOp> <aexp8>)*

<aexp8> ::= <aexp9> (<multOp> <aexp9>)*

<aexp9> ::= <prefixOp> <aexp9> | <aexp10>

<aexp10> ::= <aexp11> (<expOp> <aexp11>)*

<aexp11> ::= <unaryExpression> <postfixOp>?

<unaryExpression> ::= <elemExpression> | '(' <expression> ')'

<elemExpression> ::= <funcApplication> \alt <cellExpression> \alt <identifier> \alt <scalar> \alt <booleanLiteral> \alt <matrix> \alt <stringLiteral> \alt <anonymousFunc> \alt <funcReference>

<funcApplication> ::= <identifier> '(' ')'
\alt <identifier> '(' <expression> (',' <expression>)* ')'

<funcReference> ::= '@' <identifier>

<cellExpression> ::= <cellArray> | <cellArrayIndexing>

<cellArray> ::= '\{' <expression> (',' <expression>)* '\}'

<cellArrayIndexing> ::= <identifier> ('\{' <numericLiteral> '\}')+

<scalar> ::= <numericLiteral>

<matrix> ::= '[' <matrixRow> (<newlineOrSemicolon> <matrixRow>)* ']'

<matrixRow> ::= <expression> (',' <expression>)*

<prefixOp> ::= '+' | '-'

<multOp> ::= '*' | '/' | '.*' | './'

<addOp> ::= '+' | '-'

<compOp> ::= "'<'" | "'<='" | "'>'" | "'>='" | "'=='" | "'~='"

<postfixOp> ::= '.'' | '''

<expOp> ::= '.\textasciicircum'

<newlineOrSemicolon> ::= NL | ';'

\end{lstlisting}

For the sake of simplicity, we have left out most of the whitespace and line break handling.
Furthermore, we used notations known from regular expressions to shorten the grammar specification.
We use parentheses to group subexpressions and apply the following multiplicity specifiers: \code{*}, \code{+} and \code{?}.
The semantics of these specifiers can be found in \cref{tab:multiplicity}.

\begin{table}[!h]
  \centering
  \begin{tabular}{c|l}
  Multiplicity specifier & Meaning\\
  \hline
  $\langle exp \rangle$* & $\langle exp\rangle$ can appear multiple times \\
  $\langle exp\rangle$+ & $\langle exp\rangle$ can appear multiple times but at least once \\
  $\langle exp\rangle$? & $\langle exp\rangle$ can appear exactly once or not at all
  \end{tabular}
  \caption{Multiplicity specifier used to specify the Gilbert language. They are borrowed from regular expressions.}
  \label{tab:multiplicity}
\end{table}

The different \emph{aexp} non-terminals are used to model the precedence order of the mathematical operators.
An overview of the precedence can be found in \cref{tab:precedences}.
The $'$ operator denotes the transpose of the operand.

\begin{table}[!h]
  \centering
  \begin{tabular}{l|l}
  Operator class & Operators\\
  \hline
  Short circuit logical or & $||$\\
  Short circuit logical and & $\&\&$\\
  Logical or & $|$ \\
  Logical and & $\&$ \\
  Comparators & $>$, $>=$, $<$, $<=$, $==$, \textasciitilde$=$\\
  Range operator & $:$ \\
  Addition and Subtraction & $+$, $-$\\
  Multiplication and Division & $*$, $/$\\
  Prefix operator & $+$, $-$\\
  Exponentiation & \textasciicircum\\
  Postfix operator & $'$
  \end{tabular}
  \caption{Precedence order of mathematical operators in ascending order.}
  \label{tab:precedences}
\end{table}

The non-terminals \emph{identifier}, \emph{numericLiteral}, \emph{stringLiteral} and \emph{booleanLiteral} are tokens generated by the lexer. The corresponding tokens are defined by regular expressions and can be found in \cref{tab:tokens}.

\begin{table}[!h]
  \centering
  \begin{tabular}{l|l}
  Token name & Regular expression\\
  \hline
  identifier & [a-zA-Z] [a-zA-Z0-9\_]*\\
  numericLiteral & [+-]? [0-9]+ (.[0-9]*)? ([eE][+-]?[0-9]+)? | [+-]? .[0-9]+ ([eE][+-]?[0-9]+)? \\
  stringLiteral & "[\textasciicircum"]*" | '[\textasciicircum']*'\\
  booleanLiteral & true | false
  \end{tabular}
  \caption{Definition of generated tokens by regular expressions.}
  \label{tab:tokens}
\end{table}

\subsection{Parser}

In order to implement Gilbert, a parser had to be selected which is powerful enough to parse the Gilbert language.
Luckily, the language is rather simple and since none of the production rules is left recursive, we can use a simple LL(n) parser.
This aspect becomes important when we justify our implementation choices in \cref{cha:implementation}.

The attentive reader will have noticed that our language specification contains an ambiguity.
Consider, for example, the following valid MATLAB code \code{A' + B'} where \code{A} and \code{B} are matrices.
The code should produce the sum of two transposed matrices.
However, on the lexer level the two transpose operators will be recognized as the enclosing apostrophes of a string literal.
Therefore, the lexer will produce an identifier token followed by a string literal token.
Eventually, these tokens will cause a parser error, because there is no production rule consuming such a combination of tokens.

In order to solve this problem, we have to differentiate a transpose operator from an apostrophe belonging to a string literal.
Since the transpose operator always follows after an expression, we can check that the previously detected token is an identifier, closing parentheses, closing bracket or closing brace.
If one of these cases is true, then the apostrophe is considered to be a transpose operator.
Otherwise, it belongs to a string literal.

\subsection{Gilbert Typing}
\label{cha:gilberttyping}

Program development in general is an error-prone and time-consuming task.
Besides the actual development phase, it usually involves several iterations of bug fixing.
In order to reduce the number of pitfalls a programmer can fall into, typing systems were developed.
Typing systems assign a \emph{type} to language constructs such as variables, functions and expressions.
That way, the system gives meaning to an otherwise vacuous program.
In the memory of a computer, everything is represented as a sequence of bits, no matter whether it is an instruction code, memory address, character, boolean or floating-point number.
For a computer there is no way to intrinsically differentiate between the different meanings without an additional hint.
This hint comes in the form of types.
Knowing that a bit sequence represents a floating-point number, the computer is aware of the valid values and operations and can check for its correct usage.

The pursued goal in typing theory is to develop a system which detects erroneous program behavior and is at the same time \emph{sound} and \emph{complete}.
Soundness means that if a program passes the typer, then it behaves correctly.
Completeness means that if a program behaves correctly, then it will pass the typer.
However, it turned out that typing systems need to be extremely sophisticated in order to detect non-trivial errors and as a result they are often undecidable for non toy example languages.
For example, consider the division operation.
The code \code{1/0} would be well-typed by almost all common type checkers, because integers are divisible.
However, the code will cause a runtime error because of division by zero.
In order to detect this type of error, the type checking would have to be far more detailed.
Therefore, the constraints are usually relaxed.
In general, current type systems can already detect many different errors but they are still incomplete and only partially sound.

Type checking can be distinguished into two categories: \emph{static} and \emph{dynamic} type checking.
Static type checkers work on the source code and assign a type to each expression at compile-time.
If it detects any typing incompatibilities, such as providing the wrong arguments to a function call, assigning conflicting data types or to apply not supported operations, the type checker will alert the programmer.
Most type checkers are designed to act conservatively, meaning that they relax the constraints of soundness and completeness for the sake of decidability.
It is easy to see that the typing problem can be reduced to the halting problem, if soundness and completeness are assumed.
For this purpose consider \cref{lst:typingHaltingProblem}.
In order to decide whether this code is well- or ill-typed, the typer has to decide whether \code{f} halts.
Since the halting problem is undecidable, also the set of programs producing a runtime type error is undecidable.
\begin{listing}[!h]
  \begin{CenteredBox}
    \begin{lstlisting}[language=C++,
    commentstyle=\color{black},
    stringstyle=\color{black},
    keywordstyle=\color{black}\bfseries,
    ]
if(willHalt(f)){
  code_with_type_error;
}else{
  code_without_type_error;
}
    \end{lstlisting}
  \end{CenteredBox}
  \caption{In order to type this code fragment, the typer has to solve the halting problem.}
  \label{lst:typingHaltingProblem}
\end{listing}

In contrast to static typing, dynamic type checkers enrich each object with some kind of type tag which is used to check type compatibility at runtime.
However, possible errors are only recognized after the corresponding code has been executed.
A more thorough discussion about the advantages and disadvantages of both paradigms follows in \cref{sec:staticVSDynamic}.
In practice, there is hardly any static typing system which does not rely at least partially on dynamic typing as well.
Dynamic downcasts, for instance, as they are common in \code{C++}, can only be implemented by checking whether the underlying type is the target type or a subtype of it.

\subsection{Static Typing vs. Dynamic Typing}
\label{sec:staticVSDynamic}

Static type checking analyzes the program prior to execution to detect errors.
This approach has the advantage that possible programming mistakes are caught early in the development process.
An assignment of a string to a double would be an example for such a mistake.
As indicated by \cite{westland:jss2002a}, who investigated the influence of errors during the software development process, unfixed errors become exponentially more costly with each phase.
Therefore, it is important to detect and correct errors as soon as possible.
Static typing usually requires the user to specify types explicitly in the source code, because the language lacks type inference or has ambiguities which prevent the type inference from inferring the correct types.
Java, for example, does not have type inference and therefore the user has to specify types redundantly.
In line $1$ of \cref{lst:javaTypeAnnotation}, it is obvious that we have to specify twice the type \code{Object}, even though this type can easily be deduced from the right side of the assignment.
Furthermore, in line $4$ we see an addition of two integers.
It is clear that the result is an integer and thus the \code{int} type specifier is dispensable.
\begin{listing}[!h]
  \begin{CenteredBox}
    \begin{lstlisting}[language=C++,
      commentstyle=\color{black},
      stringstyle=\color{black},
      keywordstyle=\color{black}\bfseries,
      ]
Object obj = new Object();
int a = 1;
int b = 0;
int c = a + b;      
    \end{lstlisting}
  \end{CenteredBox}
  \caption{Type annotations in Java.}
  \label{lst:javaTypeAnnotation}
\end{listing}

Proponents of static typing emphasize that explicit type information, as they occur in Java and many other programming languages, documents the code.
It is easier for a programmer to use existing code if he can identify function arguments and their types with one glance, for instance. 
Additionally, it is possible for the compiler to apply sophisticated optimization techniques, if it knows the types.
By operating on these values, the compiler could, for example, use more efficient machine instructions for floating-point calculations.
Furthermore, it is possible to substitute virtual function calls for direct calls, if the actual object type is known.
Another benefit of static typing is the increased type safety.
After passing the type checker, the program is guaranteed to fulfill for all inputs some set of type safety properties.
This guarantee frees the runtime from checking the safety properties and thus the program can be executed more efficiently.
Type information additionally helps to provide a better programming experience in an IDE by offering type dependent context help.
If the IDE knows the type of a variable, then it can tell the programmer which methods are supported by this type, for instance.
And last but not least, explicit typing permits a better abstraction of functionality and, thus, increases modularity.
Interfaces can be defined to orchestrate the interaction between several software components allowing them to be developed independently from each other.

In contrast to these arguments, advocates of dynamic typing argue that their approach is more vivid and better suited for prototyping, because the static typing approach is too rigid.
These aspects come especially into effect for programs in a highly dynamic environment with unknown or quickly changing requirements such as data integration.
Another advantage is that the compile time is reduced because of fewer passes the compiler has to go through.
However, the avoided type checks will then be realized within the runtime which adds overhead.
But the dynamic nature allows interpreters to dynamically load code more quickly, because all type checkings are deferred until its actual execution.
Moreover, dynamic languages support duck typing as a powerful tool to write reusable code.
The term duck typing originates from the poet James Whitcomb Riley, who stated: \enquote{When I see a bird that walks like a duck and swims like a duck and quacks like a duck, I call that bird a duck.}~\cite{heim:2007a}.
Essentially, duck typing means that not the actual type of an object decides whether a program is well- or ill-typed but the set of supported methods and its properties.
For example, consider a function which calls a method \code{count} on its given parameter.
Then all function calls are valid which are called with an argument having a method \code{count}, independently of its actual type.
That way, the programmer can write code which is applicable for a wide variety of types without having to specify them explicitly.
\cite{ousterhout:c1998a} even claims that static typed languages do not guarantee a higher type safety than dynamic typed languages.
Furthermore, he states that they are more verbose and it is difficult to write reusable code.
However, it is unclear in which way reducing language features leads to a more powerful and expressive language. 

The debate of whether statically or dynamically typed languages are superior has almost reached religious character.
It is likely that none of the approaches alone solves all the problems.
Instead, a combination of the strengths of both paradigms promises the best results~\cite{meijer:2004a}.

We are in the unfortunate situation that MATLAB belongs to the class of dynamically typed languages and Gilbert requires type information for its parallel execution.
The parallel data processing systems, used to run Gilbert programs, have to know which data types are passed from one worker node to another.
Therefore, the MATLAB language has to be enriched with type information.

It exists research of how to add explicit type information to MATLAB.
For example, \cite{hendren:2011a} introduced the special keyword \emph{atype} to MATLAB which is understood and interpreted by an extended compiler.
The \emph{atype} keyword basically acts as a type annotation and can be implemented within a special library or as a weaver.
Even though this approach seems quite promising, we decided to opt for a more transparent mechanism, namely type inference.
Type inference has the advantage that the MATLAB user is not bothered by having to add explicit type information and thus can continue writing his code in the usual fashion.
In case that the type inference algorithm cannot properly infer the types, there has to be a way to resolve this problem.
We decided to pursue a similar approach as \cite{furr:2009a}.
\cite{furr:2009a} added type information to Ruby by adding special comments to the respective code sections.
Thereby, the code does not break with the Ruby standard and still contains type information.
As typing system, we use the Hindley-Milner (HM) type system~\cite{hindley:tams1969a,milner:jcss1978a} and a slightly derived form of algorithm W~\cite{damas:1982a} for type inference.
Algorithm W will be described in the next section in detail.
Even though there exist more powerful type systems than the HM type system, it has the appealing charm that the algorithm W is sound, complete and decidable with respect to the type system.
Furthermore, it has proven to type several algorithms implemented within Gilbert correctly.

\subsection{Hindley-Milner Type Inference}
\label{sec:hmInference}

The Hindley-Milner (HM) type inference assigns types to expressions.
It was initially developed to type functional languages.
In fact, HM type inference was first implemented as the typing part of the programming language ML.
HM types the expressions of the lambda calculus enriched with the \code{let}-expression.
For a detailed review of the lambda calculus, the interested reader is referred to the article of \cite{cardone:hhl2006a}.

The set of expressions $e$ contains variables, function applications, function abstractions and \code{let}-expressions.
The formal definition of $e$ is
\begin{eqnarray*}
e &= & x\quad\text{(Variable)}\\
&|& e\ e\quad\text{(Application)}\\
&|& \lambda x. e\quad\text{(Abstraction)}\\
&|& \code{let } x = e \code{ in } e
\end{eqnarray*}
For those unconversant with lambda expressions, we will quickly revise them.
The function application is written as $e_1\ e_2$ whereas $e_1$ denotes a function and $e_2$ the function argument which is applied to the function body.
The function abstraction $\lambda x.e$ is the equivalent of an anonymous function, as it is known from many program languages.
It is initiated with a $\lambda$ followed by its function parameter $x$ and the function body $e$.
The let-expression $\code{let } x = e_1 \code{ in } e_2$ introduces a new variable $x$, having the value $e_1$, into the context $\Gamma$.
This variable can be used within the expression $e_2$.
The semantic of the assignment is that every occurrence of $x$ in $e_2$ is replaced by $e_1$.
The context $\Gamma$ contains all type information so far known and is a mapping from variables to types.
A more precise definition is given in a later paragraph.

One might think that the lambda calculus is only a toy language and that the set of expressions is not expressive enough to represent any common programming problem.
However, it turned out that the lambda calculus is Turing complete and thus any programming language can be reduced to it.
Therefore, it is enough to reason about the typing aspects of these expressions.

Even though Gilbert is not a functional language, we can use the HM type inference to compute the types at compile-time.
The only difference to the above defined set of expressions is that Gilbert does not have a \code{let}-expression.
Instead, it has ordinary assignments of the form $x = e$.
The notion of the assignment is similar to the \code{let}-expression.
An assignment adds a new variable $x$ with the value $e$ to the context.

HM type inference distinguishes between two sorts of types, \emph{mono}- and \emph{polytypes}.
Monotypes $\tau$ are defined as follows:
\begin{eqnarray*}
\tau &=& \alpha\quad\text{(Variable)}\\
&|& D\ \tau\ \ldots \tau\quad\text{(Application)}
\end{eqnarray*}

A monotype always denotes a concrete type.
If the type is known, it can be a type constant, such as \code{int}, \code{float} or \code{string}, or a parametric type.
A parametric type takes itself type arguments to form a concrete type.
An example of a parametric type is the \code{Set} or the \code{List} which can be instantiated with arbitrary element types.
Type constants are a special case of the application rule with an empty list of arguments.
If the type is not yet known but can only be a single type, then a type variable $\alpha$ is used.
Often, it can be the case that these type variables are replaced with known types at a later stage of the type inference.

In contrast to monotypes, polytypes $\sigma$ denote multiple types.
They are defined by
\begin{eqnarray*}
\sigma &=& \tau \\
&|& \forall \alpha .\sigma
\end{eqnarray*}

A type of the form $\forall \alpha.\sigma$ denotes the set of all types where $\alpha$ is substituted by a concrete type $\tau$ within $\sigma$.
Consider, for example, the tuple access function \code{fst} which takes a tuple and returns the first entry.
The function \code{fst} should be applicable to all different types of tuples, for example $(\code{int}, \code{int})$, $(\code{string}, \code{float})$, etc.
If \code{fst} has a monotype, then it will only work for one concrete tuple type.
Therefore, \code{fst} has a polytype.
The actual type of \code{fst} is $\forall \alpha,\beta . (\alpha, \beta) \rightarrow \alpha$.
Depending on the applied tuple argument the quantified type variables are instantiated respectively.
Polytypes permit to implement generic functions in a type safe manner and that is called parametric polymorphism.
It is important to note that polytypes introduce ambiguities with respect to the calculated type.
An expression having the type $\forall \alpha. \alpha$ also has the type $\code{int}$, for example.
Thus, the latter type would be a valid inferred type, even though the first one is more general.
In order to dissolve this ambiguities, the typing system always looks for the most general type.

Usually, expressions depend on other expressions and thus their types.
Therefore, a type dictionary is maintained where types of expressions are stored, which already have been seen while parsing the code.
And that is exactly what the context $\Gamma$ is used for.
$\Gamma$ contains a list of pairs $x:\sigma$, which are called assumptions.
The formal definition of $\Gamma$ is
\begin{eqnarray*}
\Gamma &=& \Gamma, x:\sigma \\
&|& \epsilon\ \text{(empty)}
\end{eqnarray*}

Having defined the context, expressions and types, the typing judgment $\Gamma \vdash x : \sigma$ can be explained.
The typing judgment can be interpreted that given the context $\Gamma$ the variable $x$ has the type $\sigma$.
The way how to come up with these judgments is defined by deduction rules.
Before taking a closer look at them, some auxiliary functions are first introduced.

An important distinction in an HM type expression is the state of the type variables.
In general, type variables can appear as \emph{free} or as \emph{bound} variables.
A type variable $\alpha$ is called bound if it occurs in an expression of the form $\forall \alpha. \tau$.
$\forall$ binds the subsequent variable in the context of the expression.
All variables which are not bound are free.
We can define a function $free$ which calculates the set of free type variables in a type expression and in the type context.
\begin{eqnarray*}
free(\alpha) &=& \{\alpha\}\\
free(D\ \alpha_1\ \ldots\ \alpha_n) &=& \bigcup_{i=1}^n free(\alpha_i)\\
free(\forall \alpha. \tau) &=& free(\tau) \setminus \{\alpha\}\\
free(\Gamma) &=& \bigcup_{x:\sigma \in \Gamma} free(\sigma)
\end{eqnarray*}

\subsection{Algorithm W}

The algorithm W was initially developed by \cite{damas:1982a} and allows to solve the HM type inference in almost linear time with respect to the size of the source code....
% Thus, it is also applicable to large programs.
% A modified version of the original algorithm which incorporates side effects into the deduction rules is presented.
% Even though these side effects contradict the purity of logical deduction rules, it allows to express the algorithm concisely.
% The algorithm W is defined by the following deduction rules:

% \begin{prooftree}
%   \AxiomC{$x : \sigma \in \Gamma$}
%   \AxiomC{$\tau = inst(\sigma)$}
%   \LeftLabel{Variable:\quad}
%   \BinaryInfC{$\Gamma \vdash x : \tau$}
% \end{prooftree}

% \begin{prooftree}
%   \AxiomC{$\Gamma \vdash e_0 : \tau_0$}
%   \AxiomC{$\Gamma \vdash e_1 : \tau_1$}
%   \AxiomC{$\tau^\prime=newvar$}
%   \AxiomC{$unify(\tau_0, \tau_1 \rightarrow \tau^\prime)$}
%   \LeftLabel{Application:\quad}
%   \QuaternaryInfC{$\Gamma \vdash e_0 e_1 : \tau^\prime$}
% \end{prooftree}

% \begin{prooftree}
%   \AxiomC{$\tau = newvar$}
%   \AxiomC{$\Gamma,x:\tau \vdash e : \tau^\prime$}
%   \LeftLabel{Abstraction:\quad}
%   \BinaryInfC{$\Gamma \vdash \lambda x.e:\tau \rightarrow \tau^\prime$}
% \end{prooftree}

% \begin{prooftree}
%   \AxiomC{$\Gamma \vdash e_0 : \tau$}
%   \AxiomC{$\Gamma, x : \overline{\Gamma}(\tau) \vdash e_1 : \tau^\prime$}
%   \LeftLabel{Assignment:\quad}
%   \BinaryInfC{$\Gamma \vdash x = e_0; e_1 : \tau^\prime$}
% \end{prooftree}

% Each rule has the form
% \begin{tabular}{c}
% Premise\\
% \hline
% Conclusion
% \end{tabular}, which means that if the premise can be proven, then the conclusion is true.

% Let us go step by step through the different deduction rules of the algorithm.
% The first rule says that a variable $x$ has the type $\tau$, if an assumption of $x$ in $\Gamma$ whose type $\sigma$ can be specialized to $\tau$ is found.
% The function $inst$ does the specialization of the type.
% A type is specialized by removing all quantifiers and replacing the bound variables by fresh monotype variables.

% The second rule deals with function applications.
% In order to show that a function application has the type $\tau^\prime$, we first have to look at the function type $\tau_0$ and the argument type $\tau_1$.
% In order to fulfill the conclusion, we know that the type of $e_0$ has to be a function with an argument of type $\tau_1$ and some result type $\tau^\prime$.
% If the type $\tau_0$ is equivalent to this function type $\tau_1\rightarrow\tau^\prime$, we can deduce that the resulting type is $\tau^\prime$.
% The equivalence is tested by the $unify$ function.
% This function takes two types and tries to find the most general type unifying both arguments.

% The $unify$ function is an essential part of the typing system, since it incorporates all constraints imposed by the different expressions in order to find the most general type validating the program.
% The working principle can be best understood by looking at its pseudocode in \cref{lst:unify}.
% \begin{listing}[!h]
%   \begin{CenteredBox}
%     \begin{lstlisting}[language=ML,
%     commentstyle=\color{black},
%       stringstyle=\color{black},
%       keywordstyle=\color{black}\bfseries,
%       morekeywords={def, return},
%       deletekeywords={type, of, with, and}
%     ]
% def unify(typeA: Type, typeB: Type): Type = {
%   val tA = resolve(typeA)
%   val tB = resolve(typeB)

%   if( at least one term is a type variable){
%     return union(tA, tB)
%   }
%     else if( both type expressions are of the form D p1 ... pn with 
%     the same D and the number of arguments){
%     return D(unify(tA.p1, tB.p1), ..., unify(tA.pn, tb.pn))
%   } else{
%     return typeMismatch
%   }
% }
%     \end{lstlisting}
%   \end{CenteredBox}
%   \caption{$Unify$ function.}
%   \label{lst:unify}
% \end{listing}

% First of all, both type expressions are resolved.
% This means that type variables, which are contained as sub expressions, are looked up in the type variable assignment dictionary.
% This dictionary maintains the current assignments between variables and their types.
% If a type variable has an assigned type, then this type is substituted for the corresponding type variable.
% If at least one of the resulting type expressions is still a type variable, then we can simply construct the union of both types.
% The union function returns the more concrete type of both arguments and updates the dictionary correspondingly.
% Thus, if both arguments are type variables, then one of them is returned.
% If only one argument is a type variable, then the other argument is returned.

% If none of the resolved type expressions is a type variable, then they have to be a type application.
% Applications can only be unified if they are constructed by the same function symbol and if they have the same number of arguments.
% If this condition is fulfilled, the arguments can be unified.
% Depending on the unifiability of the arguments, the overall type application is unifiable or not.

% If both type expressions are constructed by different type applications, then the types are impossible to unify.
% Consequently, a type mismatch error will be returned.

% The third rule allows to type lambda abstractions or in Gilbert's case concrete and anonymous functions.
% In order to show that a lambda abstraction $\lambda x.e$ has the type $\tau\rightarrow \tau^\prime$, a new type variable for the unknown parameter type is introduced.
% After adding this assumption to the context, the type of the function body $e$ has to be calculated.
% If the body $e$ has the type $\tau^\prime$, then it can be concluded that the lambda abstraction has the type $\tau \rightarrow \tau^\prime$.

% The last deduction rule covers assignments.
% An assignment simply binds a variable name $x$ to an expression value.
% Since this expression value has a type $\tau$, a new assumption $x:\tau$ can be added to the context.
% The type inference for the subsequent expressions is continued with the updated type context.
% That is expressed by the deduction rule.
% In fact, there is a slight difference.
% Instead of adding the assignment $x:\tau$, the $x:\overline{\Gamma}(\tau)$ is added to the context.
% $\overline{\Gamma}(\tau)$ is the generalization of $\tau$.
% The generalization quantifies all free monotypes of $\tau$ not bound in $\Gamma$.
% That way, the most general type is found.
% \begin{eqnarray*}
%   \overline{\Gamma}(\tau) &=& \forall \overline{\alpha}. \tau\\
%   \text{with}\quad\overline{\alpha} &=& free(\tau) \setminus free(\Gamma)
% \end{eqnarray*}

\subsection{Function Overloading}

MATLAB's basic operators, such as \code{+}, \code{-}, \code{/} and \code{*}, for example, are overloaded.
They can be applied to matrices, scalars as well as mixture of both data types.
That makes it very convenient to express mathematical problems, but from a programmer's point of view it causes some hardships.
Originally, HM cannot deal with overloaded functions properly, because it assumes that each function has an unique type.
In order to extend HM's capabilities, we allowed each function symbol to have a list of signatures.
In the case of \code{+}, the list of signatures would consist of 
\begin{eqnarray*}
matrix[double] \times matrix[double] &\rightarrow& matrix[double]\\
matrix[double] \times double &\rightarrow& matrix[double]\\
double \times matrix[double] &\rightarrow& matrix[double]\\
double \times double &\rightarrow& double
\end{eqnarray*}

In order to solve the typing problem, the inference algorithm has to resolve this ambiguity.
Having complete knowledge of the argument types is enough to select the appropriate signature.
Sometimes even partial knowledge is sufficient.
However, if this information is missing, Gilbert has to apply a heuristic to make the typing expression well-formed.
The heuristic selects the first entry in the list of signatures.

Due to this heuristic, some well-typed programs will be rejected, though.
Consider, for example, the code snippet in \cref{lst:typingAmbiguity}.
In line $1$, an anonymous function $f$ is defined, which doubles its parameter.
Within the body of $f$, the overloaded \code{+} operator is used.
Since Gilbert has no additional typing information of $f$'s parameter, it has to apply the aforementioned heuristic to assign $f$ a valid type expression.
The heuristic picks the first entry which is $matrix[double] \times matrix[double] \rightarrow matrix[double]$ and , thus, $f$ has the type $matrix[double] \rightarrow matrix[double]$.
The subsequent function call \code{f(1.0)} in line $2$ will throw a typing exception, because a scalar argument cannot be applied to a function having a matrix parameter.
That is odd, because $f$ should be applicable to all data types supporting the \code{+} operator and scalar values definitely support this operation.
It has to be stated that this peculiarity is clearly a restriction of Gilbert, which limits the space of accepted programs.

\begin{listing}[!h]
  \begin{CenteredBox}
    \begin{lstlisting}[language=Matlab]
    f = @(x) x + x;
    f(1.0)
    \end{lstlisting}
  \end{CenteredBox}
  \caption{Wrongly rejected Gilbert program due to function overloading.}
  \label{lst:typingAmbiguity}
\end{listing}

\subsection{Matrix Dimension Inference}
\label{sec:MatrixDimensionInference}

Matrices constitute the elementary data type in our linear algebra environment.
Besides its element type, a matrix is also defined by its size.
In the context of program execution, knowledge about matrix sizes can help a lot to optimize the evaluation.
For instance, consider a threefold matrix multiplication $A\times B\times C$.
The multiplication can be evaluated in two different ways: $(A\times B)\times C$ and $A\times(B\times C)$.
For certain matrix sizes one way might be infeasible whereas the other way can be calculated efficiently due to the matrix size of the intermediate result $(A\times B)$ or $(B\times C)$.

To illustrate this point, assume that $A\in \mathbb{R}^{1000\times 10}, B\in \mathbb{R}^{10\times 1000}$ and $C \in \mathbb{R}^{1000\times 10}$.
Thus, the intermediate products are:
\begin{eqnarray*}
  A\times B &\in& \mathbb{R}^{1000 \times 1000}\\
  B\times C &\in& \mathbb{R}^{10 \times 10}
\end{eqnarray*}

In the first case we have to calculate a $1000 \times 1000$ matrix and in the second only a $10 \times 10$ matrix.

By knowing the matrix sizes, we can choose the most efficient strategy to calculate the requested result.
Another advantage is that we can decide whether to carry out the computation in-core or in parallel depending on the matrix sizes.
Sometimes the benefit of parallel execution is smaller than the initial communication overhead and thus it would be wiser to execute the calculation locally.
Furthermore, it can be helpful for data partitioning on a large cluster and to decide on a blocking strategy with respect to the algebraic operations.
Therefore, we extended the HM type inference to also infer matrix sizes where possible.

Gilbert's matrix type is defined as 
\begin{displaymath}
MatrixType(\underbrace{\tau}_{\text{Element type}},\underbrace{\nu}_{\text{Number of rows}},\underbrace{\nu}_{\text{Number of columns}})
\end{displaymath}
with $\nu$ being the value type:
\begin{eqnarray*}
  \nu &=& \gamma\quad\text{(Variable)} \\
  &|& \delta\quad\text{(Value)}
\end{eqnarray*}

The value type can either be a value variable or a concrete value.

Value variables always appear if we know that a certain relationship holds, but do not know the concrete values yet.
For example, when we multiply two matrices $A\in\mathbb{R}^{a\times b}$ and $B\in\mathbb{R}^{b\times c}$ we know that the result is $A\times B = C \in \mathbb{R}^{a\times c}$, without having knowledge about the actual values of $a$ and $c$.
Once we have deduced the actual values for a variable we can simply replace them.

One of these occasions is the definition of a matrix.
Gilbert implements several functions, known from Matlab: \code{eye}, \code{zeros}, \code{ones} or the \code{load} function, reading a matrix from disk.
All of these functions require to specify the number of rows and columns for the resulting matrix.
These values often serve as the source of deduction.
Another possibility are special operations which modify the size of the matrix in a deterministic way.
The sum operation, which calculates row or column sums, respectively, will reduce the dimension which is used for the summation to $1$.

The matrix size inference is incorporated into the HM type inference by adding some logic to the \code{unify} function.
Whenever we encounter a matrix type during the unification process, we call a \code{unifyValue} function on the two row and column values.
The \code{unifyValue} function works similarly to the \code{unify} function.
First, the function resolves the value expression, thus substituting value variables with their assigned values.
Then, if at least one of the resolved value expressions is still a value variable, then the union is constructed and the corresponding value variable dictionary entry is updated.
If both resolved expressions are equal, then this value is returned and otherwise a value mismatch error is thrown.

\section{Intermediate Representation}
\label{cha:intermediaterepresentation}

After parsing and typing of the source code is done, we have all the information needed to execute the program in a distributed fashion.
However, we do not compile the code directly to the respective format to let it run on a parallel data processing system.
Instead, we compile it to an intermediate representation.
Even though, the intermediate code adds an additional representation and thus more complexity to maintain, the advantages clearly outweigh the disadvantages.
Moreover, this approach is similar to the approach the JVM and the .Net frameworks pursue.
The Java and .Net code are compiled to a format which can be executed platform independently.

The additional abstraction layer allows Gilbert to apply language independent optimization in a coherent manner and independently of the actually used front end language.
This implementation aspect gives the flexibility to easily extend Gilbert to support other linear algebra languages as well.
The next natural candidate for a supported front end language would surely be R.
Moreover, a higher-level abstraction of the mathematical operations holds more potential for algebraic optimization.
One class of important transformations is the reordering of operations and the determination of the execution order.
Often, they are based on the commutative and associative property of the corresponding operations.
An obvious optimization would be the execution order of multiple matrix multiplications as indicated in \cref{sec:MatrixDimensionInference}.
Furthermore, the propagation and elimination of transpose operations offers further optimization potential.

The intermediate representation is designed with two goals in mind:
First of all, it has to be expressive enough to represent in a general way a wide variety of MATLAB programs.
Therefore, the intermediate format has to include the operational primitives of linear algebra as well as an iteration abstraction.
With these building blocks at hand, it is already possible to express a multitude of iterative machine learning algorithms.
As pointed out in \cref{sec:languageFeatures}, Gilbert does not provide a direct \code{for} or \code{while} loop, but instead the \code{fixpoint} operator.
The second design goal was simplicity.
It is of particular interest to keep the set of intermediate operators as small as possible, because it would alleviate a possible optimization step prior to execution.

\subsection{Specification}

The intermediate format consists of a set of operators to represent the different linear algebra operations.
Every operator has a distinct result type and a set of parameters which are required as inputs.
\Cref{fig:depTreeMMIdOnes} shows how a matrix multiplication between an identity matrix $Id \in \mathbb{R}^{10\times10}$ and a matrix $Ones \in \mathbb{R}^{10\times10}$ filled with $1$s would be represented.
On the level of the intermediate format we distinguish between the following types: \code{MatrixType}, \code{ScalarType}, \code{CellArrayType}, \code{StringType} and \code{FunctionType}.

\begin{figure}
  \begin{center}
    \Tree [.MatrixMult [.eye [.scalar 10 ] [.scalar 10 ] ] [.ones [.scalar 10 ] [.scalar 10 ] ] ]
  \end{center}
  \caption{Dependency tree of a matrix multiplication between an identity matrix and a matrix filled with 1s.}
  \label{fig:depTreeMMIdOnes}
\end{figure}

The matrix type is the elementary type and represents all sorts of matrices.
A matrix contains information about its size, namely the number of rows and columns, and about its element type.
The element type can be any supported type but currently only operations for double or boolean matrices are implemented.

The scalar type is a superset containing the \code{DoubleType} and \code{BooleanType}.
Usually, the double type is used for all linear algebra operations.
Boolean scalars are mainly used for convergence criteria of iterations.

The cell array type contains a list of types specifying the types of its cell entries.
Cell arrays are usually used to return multiple values from a function.
Thus, they can conceptually be seen as tuples.
In contrast to the MATLAB implementation, Gilbert only supports $1$-dimensional cell arrays.

The string type was included to be able to load matrices from HDFS or local disk.
In order to load a matrix, the user has to define the location of the data, given as a string.
Besides this usage, strings are usually not necessary for linear algebra operations.

Last but not least, Gilbert also supports function types.
Function types are necessary to implement second-order functions such as the fixpoint operator.
The user can pass defined functions around like any other value.
Thus, functions are considered first-class citizens in Gilbert.

The set of intermediate operators can be distinguished into three categories: \emph{Creation operators}, \emph{transformation operators} and \emph{Output operators}.
All of these categories will be explained in the following subsections.

\subsubsection{Creation Operators}

Creation operators generate or load some data depending on the used function.
A list of all supported operators with its type definition and a short explanation can be found in \cref{tab:creatingOperators}.
Each type definition consists of the input types and the result type of an operator:
\begin{displaymath}
  \underbrace{type1 \times \ldots \times typeN}_{\text{Types of input parameters}} \rightarrow \underbrace{resultType}_{\text{Type of result value}}
\end{displaymath}
Even though the input parameters for the matrix dimensions are denoted by doubles, the user should provide integer values.
Internally, the double parameters are converted to integers.
The type in brackets after the matrix type indicates the element type.

The \code{load} operator takes a path to a stored matrix on disk and the corresponding number of rows and columns as input parameters.
It then reads the stored data and loads the matrix into the program context.
The \code{eye} operator, known from Matlab, generates an identity matrix of the requested size, number of rows and columns.
The operator also supports non quadratic result shapes.
Another well-known operator is \code{zeros}.
It generates a matrix of the given size which is initialized with zeros.
And the last creation operator is \code{randn} which generates a random matrix.
\code{Randn} takes the number of rows and columns of the resulting matrix and the mean and the standard deviation of a Gauss distribution.
The specified Gauss distribution is used to generate random values for the resulting matrix.

\begin{table}
  \centering
  \begin{tabular}{l|l|l}
  Operator & Type & Explanation\\
  \hline
  \code{load} & $string \times double \times double \rightarrow matrix[double]$ & Loading a matrix from disk\\
  \code{eye} & $double \times double \rightarrow matrix[double]$& Creating an identity matrix\\
  \code{zeros} & $double \times double \rightarrow matrix[double]$ & Creating a zero matrix\\
  \code{randn} & $double \times double \rightarrow matrix[double]$ & Creating a random matrix
  \end{tabular}
  \caption{Creating operators of Gilbert, their types and a short explanation.}
  \label{tab:creatingOperators}
\end{table}

\subsubsection{Transformation Operators}

The transformation operators constitute the main abstraction of the linear algebra operations.
They group operations with similar properties and thus allow an easier reasoning and optimization of the underlying program.
The list of all transformation operators and their types can be found in \cref{tab:transformingOperators}.

\begin{table}
  \centering
  \begin{tabular}{l|l}
    Operator & Type\\
    \hline
    \code{UnaryScalarTransformation} & $scalar \times unaryScalarOp \rightarrow scalar$\\
    \code{ScalarScalarTransformation} & $scalar \times scalar \times scalarOp \rightarrow scalar$\\
    \code{Transpose} & $matrix \rightarrow matrix$\\
    \code{ScalarMatrixTransformation} & $scalar \times matrix \times smOp \rightarrow matrix$ \\
    \code{MatrixScalarTransformation} & $matrix \times scalar \times smOp \rightarrow matrix$ \\
    \code{CellwiseMatrixTransformation} & $matrix \times unaryScalarOp \rightarrow matrix$ \\
    \code{CellwiseMatrixMatrixTransformation}& $matrix \times matrix \times scalarOp \rightarrow matrix$ \\
    \code{MatrixMult} & $matrix \times matrix \rightarrow matrix$\\
    \code{VectorwiseMatrixTransformation} & $matrix \times vectorwiseOp \rightarrow matrix$ \\
    \code{AggregateMatrixTransformation} & $matrix \times aggregateOp \rightarrow scalar$ \\
    \code{FixpointIterationMatrix} & $matrix \times \left( matrix \rightarrow matrix \right) \times double \times$ \\
    &$\quad \left( matrix \times matrix \rightarrow boolean \right) \rightarrow matrix$ \\
    \code{FixpointIterationCellArray} & $cellarray \times \left( cellarray \rightarrow cellarray \right) \times double$ \\
    &$\quad \times \left(cellarray \times cellarray \rightarrow boolean\right)$\\
    &$\quad \rightarrow cellarray$
  \end{tabular}
  \caption{Transforming operators of Gilbert and their types.}
  \label{tab:transformingOperators}
\end{table}

The \code{UnaryScalarTransformation} takes a single scalar value and applies an unary operation on it.
The list of all supported operations can be found in \cref{tab:supportedOperations}.
The operation $binarize$ has the following semantics:
\begin{displaymath}
  binarize(x) : x \mapsto 
  \begin{cases}
    0 &\text{if } x = 0\\
    1 &\text{else}
  \end{cases}
\end{displaymath}

The applicable operations to the \code{ScalarScalarTransformation} depend on the input parameter types.
The operators $+,-,*,/,pow(\cdot, \cdot)$ take two double values and produce a new double value.
The comparison operators, on the contrary, produce a boolean result value.
The logical operations require two boolean values and produce a boolean value as the result.
Gilbert supports short-circuit logical operators $\&\&$ and $||$ as well as $\&$ and $|$ evaluating all of their inputs prior to computing the result.

The \code{ScalarMatrixTransformation} and \code{MatrixScalarTransformation} have an almost identical support of operations.
The only difference is that the former transformation does not allow to calculate the power of a scalar where the exponent is a matrix.

The \code{VectorwiseMatrixTransformation} applies an operation on each row vector of the given matrix.
A vectorwise operation produces a scalar value for each row vector.
Thus, the resulting type of such an operation is an one-column matrix.
Gilbert can currently retrieve the maximum and minimum of each row vector.
Furthermore, one can calculate the $2$-norm of each row vector:
\begin{displaymath}
  \norm{\left(a_{i1}, \ldots, a_{in}\right)^T}_2 = \sqrt{\sum_{j=1}^{n} a_{ij}^2}
\end{displaymath}

Similar to the vectorwise operations, the \code{AggregateMatrixTransformation} applies an operation to all matrix entries producing a single scalar result value.
The aggregation operation can compute the maximum, minimum and the sum of all matrix entries.
Furthermore, it supports the Frobenius norm:
\begin{displaymath}
  \norm{A}_2 = \sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n} a_{ij}^2}
\end{displaymath}

\begin{table}
  \centering
  \begin{tabular}{l|l}
  Operation type & Operations\\
  \hline
  $unaryScalarOp$ & $-$, $|\cdot|$, $binarize(\cdot)$\\
  $scalarOp:double \times double \rightarrow double$ & $+,-,*,/,pow(\cdot, \cdot)$\\
  $scalarOp:double \times double \rightarrow boolean$ & $>,>=,<,<=,==,$\textasciitilde$=$\\
  $scalarOp:boolean \times boolean \rightarrow boolean$ & $\&, |, \&\&, ||$\\
  $smOp:matrix[double] \times double \rightarrow matrix[double]$ & $+,-,*,/,pow(\cdot, \cdot)$\\
  $smOp:matrix[double] \times double \rightarrow matrix[boolean]$ & $>,>=,<,<=,==,$\textasciitilde$=$\\
  $smOp:matrix[boolean] \times boolean \rightarrow matrix[boolean]$ & $\&, |, \&\&, ||$\\
  $smOp:double \times matrix[double] \rightarrow matrix[double]$ & $+,-,*,/$\\
  $smOp:double \times matrix[double] \rightarrow matrix[boolean]$ & $>,>=,<,<=,==,$\textasciitilde$=$\\
  $smOp:boolean \times matrix[boolean] \rightarrow matrix[boolean]$ & $\&, |, \&\&, ||$\\
  $vectorwiseOp$ & $min, max, norm2$\\
  $aggregateOp$ & $min, max, sumAll, norm2$
  \end{tabular}
  \caption{Supported transformation operations.}
  \label{tab:supportedOperations}
\end{table}

The iteration mechanism is represented by the \code{FixpointIterationMatrix} and \\\code{FixpointIterationCellArray} operators.
The former operator is used for a fixpoint operation on matrices and the latter on cell arrays.
Both operators follow the semantics and have the same parameters as the fixpoint abstraction presented in \cref{eqn:fixpoint}.
A fixpoint operator receives an initial value, an update function, the maximum number of iterations and a convergence function.
The update function is called with the current value and returns the value for the next iteration.
The convergence function is called with the current and the previous value.
Based on these values it can check for convergence.
For further details see \cref{sec:languageFeatures}.

\subsubsection{Writing Operators}

The writing operators are used to make the computed results accessible to the user by writing them back to disk.
There exists a writing operation for each supported type.
The list of all writing operations can be seen in \cref{tab:writingOperations}.

\begin{table}[!h]
  \centering
  \begin{tabular}{l|l}
    Operator& Explanation\\
    \hline
    \code{WriteMatrix} & Writes the matrix to disk\\
    \code{WriteScalar} & Writes the scalar value to disk\\
    \code{WriteString} & Writes the string value to disk\\
    \code{WriteCellArray} & Writes the contents of the cell array to disk \\
    \code{WriteFunction} & Writes the intermediate execution plan to disk
  \end{tabular}
  \caption{Supported writing operators.}
  \label{tab:writingOperations}
\end{table}

\subsubsection{Compilation Example}

The intermediate representation is the target format of the compilation process after it has been parsed and typed.
In order to illustrate the compilation process and to get a feeling for the intermediate representation, we will compile an example program.
For this purpose, we have chosen the well-known PageRank algorithm~\cite{page:1999a}.
The Matlab implementation can be seen in \cref{lst:PageRank}.

\begin{listing}[h!]
  \begin{CenteredBox}
    \begin{lstlisting}[language=Matlab,
    commentstyle=\color{black},
      stringstyle=\color{black},
      keywordstyle=\color{black}\bfseries,
      morekeywords={ones, fixpoint},
      deletekeywords={eps},
      ]
numVertices = 10;
% load network matrix
N = load("network.csv", numVertices, numVertices);
% create the adjacency matrix
A = spones(N);
% outdegree per vertex
d = sum(A, 2);
% create the column-stochastic transition matrix
T = (diag(1 ./ d) * A)';
% initialize the ranks
r_0 = ones(numVertices, 1) / numVertices;
e = ones(numVertices, 1) / numVertices;
% update function
f = @(r) (.85 * T * r + .15 * e)
eps = 0.1;
c =  @(prev, cur) norm(prev-cur,2) <= eps
% PageRank calculation
fixpoint(r_0, f, 20, c);
    \end{lstlisting}
  \end{CenteredBox}
  \caption{Matlab PageRank implementation.}
  \label{lst:PageRank}
\end{listing}

We start at the fixpoint operation in line $20$ and build our intermediate representation bottom up.
Since the fixpoint operation is given an initial matrix, it will be compiled to a \\\code{FixpointIterationMatrix} operator.
The intermediate code can be seen in \cref{fig:irFixpoint}.

\begin{figure}[!h]
  \centering
  \Tree [.FixpointIterationMatrix r\_0 f [.scalar 20 ] c ]
  \caption{Intermediate representation of PageRank's fixpoint operation.}
  \label{fig:irFixpoint}
\end{figure}

The translation from line $20$ to the intermediate code is straight forward.
The actual compilation process would replace the variables \code{r\_0}, \code{f} and \code{c} with their actual definition.
However, for the sake of simplicity, we have left them in place and will gradually refine them.

The variable \code{r\_0} denotes the initial column vector of the PageRank iteration.
Its resulting code from the compilation process can be seen in \cref{fig:irR0}.

\begin{figure}[!h]
  \centering
  \Tree [.MatrixScalarOperation [.ones [.scalar 10 ] [.scalar 1 ] ] [.scalar 10 ] '/' ]
  \caption{Intermediate representation of PageRank's \code{r\_0}.}
  \label{fig:irR0}
\end{figure}

The update function \code{f} is defined in line $15$ as an anonymous function.
In order to represent functions in the intermediate format, we have to introduce a new operator \code{function} which takes the number of parameters and the body of the function.
Additionally, all parameters of the function are represented by special intermediate code operators: \code{MatrixParameter}, \code{ScalarParameter}, \code{StringParameter} and \code{FunctionParameter}.
Every parameter operator gets an index assigned which identifies the corresponding argument with which it has to be replaced upon instantiation of the function.
The compiled code of \code{f} can be found in \cref{fig:irF}.

\begin{figure}[!h]
  \centering
  \Tree [.function 1 [.CellwiseMatrixMatrixTransformation [.ScalarMatrixTransformation [.scalar .85 ] [.MatrixMult T [.MatrixParameter 0 ] ] '*' ] [.ScalarMatrixTransformation [.scalar .15 ] e '*' ] '+' ] ]
  \caption{Intermediate representation of PageRank's \code{f}.}
  \label{fig:irF}
\end{figure}

The matrix \code{T} is the transition matrix $T=(t)_{ij} \in \mathbb{R}^{10 \times 10}$.
The entry $t_{ij}$ indicates the probability of going from site $j$ to site $i$ if the user is currently on site $j$.
The dependency tree of the intermediate code of \code{T} is shown in \cref{fig:irT}.

\begin{figure}[!h]
  \centering
  \Tree [.Transpose [.MatrixMult [.diag [.ScalarMatrixTransformation [.scalar 1 ] [.sum A [.scalar 2 ] ] './' ] ] A ] ]
  \caption{Intermediate representation of PageRank's \code{T}.}
  \label{fig:irT}
\end{figure}

Here \code{diag} denotes a special operator which has a different behavior depending on the given argument.
If one provides a one-column matrix $d$ to \code{diag}, then a zero matrix with $d$ on its diagonal is created.
If one provides a matrix $m$ to \code{diag}, then the diagonal of $m$ is returned.
The operator \code{sum} takes a matrix and a dimension and computes the sums along the specified dimension.
Thus, the row sums will be computed in this case.

What is left to be compiled for \code{f} is the matrix \code{A}.
The compiled code can be seen in \cref{fig:irA}.

\begin{figure}[!h]
  \centering
  \Tree [.CellwiseMatrixTransformation [.load 'network.csv' [.scalar 10 ] [.scalar 10 ] ] 'binarize' ]
  \caption{Intermediate representation of PageRank's \code{A}.}
  \label{fig:irA}
\end{figure}

The Matlab function \code{spones}, which replaces nonzero sparse elements with ones while preserving the sparsity structure, is compiled into a CellwiseMatrixTransformation.

The \code{e} vector has the same representation as the initial PageRank vector \code{r\_0}.

\begin{figure}[!h]
  \centering
  \Tree [.MatrixScalarOperation [.ones [.scalar 10 ] [.scalar 1 ] ] [.scalar 10 ] '/' ]
  \caption{Intermediate representation of PageRank's \code{e}.}
  \label{fig:irE}
\end{figure}

The convergence function \code{c} is an anonymous function taking two parameters.
Its compiled version is shown in \cref{fig:irC}

\begin{figure}[!h]
  \centering
  \Tree [.function [.scalar 2 ] [.ScalarScalarTransformation [.norm [.CellwiseMatrixMatrixTransformation [.MatrixParameter 0 ] [.MatrixParameter 1 ] '-' ] [.scalar 2 ] ] [.scalar 0.1 ] '>=' ] ]
  \caption{Intermediate representation of PageRank's \code{c}.}
  \label{fig:irC}
\end{figure}

The \code{norm} operator, taking as first parameter the matrix $m$ and as second the norm parameter $p$, calculates the $p$-norm of $m$ over all matrix elements.
Internally this operator is compiled to the intermediate code shown in \cref{fig:irNorm}.

\begin{figure}[t]
  \centering
  \Tree [.function [.scalar 2 ] [.ScalarScalarTrans [.AggregateMatrixTrans [.CellwiseMatrixTrans [.MatrixScalarTrans [.MatrixParam 0 ] [.ScalarParam 1 ] 'pow' ] 'abs' ] 'sumAll' ] [.ScalarScalarTrans [.scalar 1 ] .ScalarParam 1 '/' ] 'pow' ] ]
  \caption{Intermediate representation of \code{norm}.}
  \label{fig:irNorm}
\end{figure}